{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/nfs/homedirs/ayle/guided-research/SNIP-it/bayesian')\n",
    "import sys\n",
    "sys.path.append('/nfs/homedirs/ayle/guided-research/SNIP-it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "260926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main_bayesian.py --net_type conv6 --dataset CIFAR10 --prune_criterion StructuredSNR --pruning_limit 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import data\n",
    "from main_bayesian import getModel\n",
    "import config_bayesian as cfg\n",
    "\n",
    "from main_bayesian import validate_model\n",
    "import metrics\n",
    "from uncertainty_estimation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = init_dataset('CIFAR10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_data = init_dataset('SVHN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'conv6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD STRUCTURED PRUNED MODEL\n",
    "if net_type == 'customconv6':\n",
    "    import pickle\n",
    "    with open('./checkpoints/CIFAR10/bayesian/model_conv6_bbb_relu_StructuredSNR_0.5.pt', 'rb') as f:\n",
    "         net = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/nfs/homedirs/ayle/model_conv6_0.5.pickle', 'rb') as f:\n",
    "     pre_pruned_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getModel(net_type, 3, 10, priors=None, layer_type='bbb', activation_type='relu', pre_pruned_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('./checkpoints/CIFAR10/bayesian/model_conv6_bbb_relu_StructuredSNR_0.5_during.pt'))\n",
    "net.eval()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/nfs/homedirs/ayle/mask.pickle', 'rb') as f:\n",
    "    mask = pickle.load(f)\n",
    "\n",
    "mask_keys = list(mask.keys())\n",
    "\n",
    "count = 0\n",
    "for name, module in net.named_modules():\n",
    "    if name.startswith('conv') or name.startswith('fc'):\n",
    "        module.mask = mask[mask_keys[count]]\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./checkpoints/CIFAR10/bayesian/model_conv6_bbb_relu_StructuredSNR_0.5_during.pt', 'rb') as f:\n",
    "     net = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = 0.7\n",
    "all_scores = []\n",
    "for name, module in net.named_modules():\n",
    "    if name.startswith('conv') or name.startswith('fc'):\n",
    "        scores = torch.abs(module.W_mu) / torch.log1p(torch.exp(module.W_rho)) # / module.weight.sigma\n",
    "#         scores = - torch.log1p(torch.exp(module.W_rho)) \n",
    "        all_scores.append(scores.flatten())\n",
    "all_scores = torch.cat([x for x in all_scores])\n",
    "threshold, _ = torch.topk(all_scores, int(len(all_scores)*(1-sparsity)), sorted=True)\n",
    "acceptable_score = threshold[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in net.named_modules():\n",
    "    if name.startswith('conv') or name.startswith('fc'):\n",
    "        mask = (torch.abs(module.W_mu) / torch.log1p(torch.exp(module.W_rho))) > acceptable_score\n",
    "#         mask = - torch.log1p(torch.exp(module.W_rho))  > acceptable_score\n",
    "#         mask = (- module.weight.sigma) > acceptable_score\n",
    "        module.mask = mask\n",
    "        \n",
    "        print(mask.sum().float() / torch.numel(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "trainset, testset, inputs, outputs = data.getDataset('CIFAR10')\n",
    "train_loader, valid_loader, test_loader = data.getDataloader(\n",
    "trainset, testset, valid_size, batch_size, num_workers)\n",
    "\n",
    "ood_trainset, ood_testset, ood_inputs, ood_outputs = data.getDataset('SVHN')\n",
    "ood_train_loader, ood_valid_loader, ood_test_loader = data.getDataloader(\n",
    "ood_trainset, ood_testset, valid_size, batch_size, num_workers)\n",
    "\n",
    "criterion = metrics.ELBO(len(trainset)).to(device)\n",
    "beta_type = 0.1\n",
    "epoch = 1\n",
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ens = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss, valid_acc, max_probs = validate_model(net, criterion, valid_loader, num_ens=n_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_valid_loss, _, ood_max_probs = validate_model(net, criterion, ood_valid_loader, num_ens=n_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as sk_metrics\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def calculate_auroc(correct, predictions):\n",
    "    fpr, tpr, thresholds = sk_metrics.roc_curve(correct, predictions)\n",
    "    auroc = sk_metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr)\n",
    "    return auroc\n",
    "\n",
    "\n",
    "def calculate_aupr(correct, predictions):\n",
    "    aupr = sk_metrics.average_precision_score(correct, predictions)\n",
    "    return aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = np.concatenate((np.ones_like(max_probs), np.zeros_like(ood_max_probs)))\n",
    "print(calculate_auroc(corrects, np.concatenate((max_probs, ood_max_probs))))\n",
    "print(calculate_aupr(corrects, np.concatenate((max_probs, ood_max_probs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epi, all_ale = 0, 0\n",
    "\n",
    "for sample in test_loader:\n",
    "    pred, epi_norm, ale_norm = get_uncertainty_per_batch(net, sample[0], T=25, normalized=True)\n",
    "    pred, epi_soft, ale_soft = get_uncertainty_per_batch(net, sample[0], T=25, normalized=False)\n",
    "    \n",
    "    all_epi += epi_norm.mean(0)\n",
    "    all_ale += ale_norm.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_epi.mean())\n",
    "print(all_ale.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_all_epi, ood_all_ale = 0, 0\n",
    "\n",
    "for sample in ood_test_loader:\n",
    "    pred, epi_norm, ale_norm = get_uncertainty_per_batch(net, sample[0], T=25, normalized=True)\n",
    "    pred, epi_soft, ale_soft = get_uncertainty_per_batch(net, sample[0], T=25, normalized=False)\n",
    "    \n",
    "    ood_all_epi += epi_norm.mean(0)\n",
    "    ood_all_ale += ale_norm.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ood_all_epi.mean())\n",
    "print(ood_all_ale.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6441222\n",
    "1.8379018\n",
    "\n",
    "0.69216275\n",
    "1.8008404"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
