{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/nfs/homedirs/ayle/guided-research/SNIP-it/glow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python train.py CELEBA --img_size 32 --channels 3 --batch 64 --prune_criterion SNIPit --pruning_limit 0.5 --local_pruning --checkpoint checkpoint/model_dataset=CELEBA_criterion=EmptyCrit_sparsity=0.0_local=False.pt  # --optim_checkpoint checkpoint/optim_dataset=train_criterion=EmptyCrit_sparsity=0.0_local=False.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from math import log, sqrt, pi\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from model import Glow\n",
    "from glow.Johnit import Johnit\n",
    "from glow.John import John\n",
    "from glow.SNIPit import SNIPit\n",
    "from glow.criterions.StructuredEFGit import StructuredEFGit\n",
    "from glow.criterions.SNAP import SNAP\n",
    "from glow.train import get_celeba_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch = 32\n",
    "n_flow = 32\n",
    "n_block = 4\n",
    "no_lu = False\n",
    "affine = False\n",
    "n_bits = 5\n",
    "lr = 1e-5\n",
    "img_size = 32\n",
    "channels = 3\n",
    "temp = 0.7\n",
    "n_sample = 20\n",
    "iterations = 1000\n",
    "n_bins = 2.0 ** n_bits\n",
    "\n",
    "pruning_limit = 0.9\n",
    "local_pruning = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(path, batch_size, image_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.ImageFolder(path, transform=transform)\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "    loader = iter(loader)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(loader)\n",
    "\n",
    "        except StopIteration:\n",
    "            loader = DataLoader(\n",
    "                dataset, shuffle=True, batch_size=batch_size, num_workers=4\n",
    "            )\n",
    "            loader = iter(loader)\n",
    "            yield next(loader)\n",
    "\n",
    "\n",
    "def calc_z_shapes(n_channel, input_size, n_flow, n_block):\n",
    "    z_shapes = []\n",
    "\n",
    "    for i in range(n_block - 1):\n",
    "        input_size //= 2\n",
    "        n_channel *= 2\n",
    "\n",
    "        z_shapes.append((n_channel, input_size, input_size))\n",
    "\n",
    "    input_size //= 2\n",
    "    z_shapes.append((n_channel * 4, input_size, input_size))\n",
    "\n",
    "    return z_shapes\n",
    "\n",
    "\n",
    "def calc_loss(log_p, logdet, image_size, n_bins, channels):\n",
    "    # log_p = calc_log_p([z_list])\n",
    "    n_pixel = image_size * image_size * channels\n",
    "\n",
    "    loss = -log(n_bins) * n_pixel\n",
    "    loss = loss + logdet + log_p\n",
    "\n",
    "    return (\n",
    "        (-loss / (log(2) * n_pixel)).mean(),\n",
    "        (log_p / (log(2) * n_pixel)).mean(),\n",
    "        (logdet / (log(2) * n_pixel)).mean(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_single = Glow(\n",
    "    channels, n_flow, n_block, affine=affine, conv_lu=not no_lu\n",
    ")\n",
    "model = nn.DataParallel(model_single)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_path = \"checkpoint/model_dataset=CELEBA_criterion=Johnit_sparsity=0.5_local=True.pt\"\n",
    "# model_path = \"/nfs/students/ayle/guided-research/glow/checkpoints/model_criterion=EmptyCrit_sparsity=0.0_local=False.pt\"\n",
    "# # model_path = \"/nfs/students/ayle/guided-research/glow/checkpoints/model_criterion=Johnit_sparsity=0.9_local=True.pt\"\n",
    "model_path = \"checkpoint/model_dataset=CELEBA_criterion=EmptyCrit_sparsity=0.0_local=False.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.module.named_modules():\n",
    "#     if name + \".weight\" in model.module.mask:\n",
    "# #         torch.nn.init.kaiming_normal_(\n",
    "# #                         module.weight.data, mode='fan_in', nonlinearity='relu'\n",
    "# #                     )\n",
    "#         torch.nn.init.kaiming_normal_(module.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name)\n",
    "#     print((param == 0).float().sum() / torch.numel(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/nfs/students/ayle/guided-research/FASHION-jpg/training\"\n",
    "\n",
    "# dataset = iter(sample_data(path, batch, img_size))\n",
    "# n_bins = 2.0 ** n_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASHION train data\n",
    "path = \"/nfs/students/ayle/guided-research/FASHION-jpg/training\"\n",
    "dataset = sample_data(path, batch, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELEBA train data\n",
    "dataset = get_celeba_loaders('/nfs/students/ayle/guided-research/', batch, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 train data\n",
    "path  =\"/nfs/students/ayle/guided-research/CIFAR-10-images/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROPit\n",
    "criterion = Johnit(limit=pruning_limit, model=model.module, generative=True, nbins=n_bins, img_size=img_size, channels=channels, loss_f=calc_loss)\n",
    "criterion.prune(pruning_limit, train_loader=dataset, local=local_pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROP\n",
    "criterion = John(limit=pruning_limit, model=model.module, generative=True, nbins=n_bins, img_size=img_size, channels=channels, loss_f=calc_loss)\n",
    "criterion.prune(pruning_limit, train_loader=dataset, local=local_pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPit\n",
    "criterion = SNIPit(limit=pruning_limit, model=model.module, generative=True, nbins=n_bins, img_size=img_size, channels=channels, loss_f=calc_loss)\n",
    "criterion.prune(pruning_limit, train_loader=dataset, local=local_pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "dataset = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "z_sample = []\n",
    "z_shapes = calc_z_shapes(channels, img_size, n_flow, n_block)\n",
    "for z in z_shapes:\n",
    "    z_new = torch.randn(n_sample, *z) * temp\n",
    "    z_sample.append(z_new.to(device))\n",
    "\n",
    "with tqdm(range(iterations)) as pbar:\n",
    "    for i in pbar:\n",
    "        image, _ = next(dataset)\n",
    "        image = image.to(device)\n",
    "        \n",
    "        image = image * 255\n",
    "        \n",
    "        if n_bits < 8:\n",
    "            image = torch.floor(image / 2 ** (8 - n_bits))\n",
    "\n",
    "        image = image / n_bins - 0.5\n",
    "\n",
    "        model.module.apply_weight_mask()\n",
    "\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                log_p, logdet, _ = model.module(\n",
    "                    image + torch.rand_like(image) / n_bins\n",
    "                )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                utils.save_image(\n",
    "                    model_single.reverse(z_sample).cpu().data,\n",
    "                    f\"sample/{str(i + 1).zfill(6)}.png\",\n",
    "                    normalize=True,\n",
    "                    nrow=10,\n",
    "                    range=(-0.5, 0.5),\n",
    "                    )\n",
    "                \n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            log_p, logdet, _ = model(image + torch.rand_like(image) / n_bins)\n",
    "\n",
    "        logdet = logdet.mean()\n",
    "\n",
    "        loss, log_p, log_det = calc_loss(log_p, logdet, img_size, n_bins, channels=channels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        # warmup_lr = args.lr * min(1, i * batch_size / (50000 * 10))\n",
    "        warmup_lr = lr\n",
    "        optimizer.param_groups[0][\"lr\"] = warmup_lr\n",
    "        optimizer.step()\n",
    "\n",
    "        model.module.apply_weight_mask()\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Loss: {loss.item():.5f}; logP: {log_p.item():.5f}; logdet: {log_det.item():.5f}; lr: {warmup_lr:.7f}\"\n",
    "        )\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                utils.save_image(\n",
    "                    model_single.reverse(z_sample).cpu().data,\n",
    "                    f\"sample/{str(i + 1).zfill(6)}.png\",\n",
    "                    normalize=True,\n",
    "                    nrow=10,\n",
    "                    range=(-0.5, 0.5),\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASHION\n",
    "path = \"/nfs/students/ayle/guided-research/FASHION-jpg/testing\"\n",
    "dataset = iter(sample_data(path, batch, img_size))\n",
    "len_dataset = len(datasets.ImageFolder(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10\n",
    "path  =\"/nfs/students/ayle/guided-research/CIFAR-10-images/test\"\n",
    "dataset = iter(sample_data(path, batch, img_size))\n",
    "len_dataset = len(datasets.ImageFolder(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELEBA test set\n",
    "test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.CenterCrop(img_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "test_set = datasets.CelebA(\n",
    "    '/nfs/students/ayle/guided-research/',\n",
    "    split='test',\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "dataset = iter(DataLoader(test_set, shuffle=False, batch_size=batch, num_workers=4))\n",
    "len_dataset = len(datasets.CelebA(\n",
    "            '/nfs/students/ayle/guided-research/',\n",
    "            split='test',\n",
    "            download=True\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### EVALUATION\n",
    "\n",
    "n_bins = 2.0 ** n_bits\n",
    "\n",
    "z_sample = []\n",
    "z_shapes = calc_z_shapes(channels, img_size, n_flow, n_block)\n",
    "for z in z_shapes:\n",
    "    z_new = torch.randn(n_sample, *z) * temp\n",
    "    z_sample.append(z_new.to(device))\n",
    "    \n",
    "cum_log_p = []\n",
    "\n",
    "with tqdm(range(int(len_dataset / batch))) as pbar:\n",
    "    for i in pbar:\n",
    "        image, _ = next(dataset)\n",
    "        image = image.to(device)\n",
    "\n",
    "        image = image * 255\n",
    "\n",
    "        if n_bits < 8:\n",
    "            image = torch.floor(image / 2 ** (8 - n_bits))\n",
    "\n",
    "        image = image / n_bins - 0.5\n",
    "\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                log_p, logdet, _ = model.module(\n",
    "                    image\n",
    "                )\n",
    "                cum_log_p.append(log_p.cpu().detach().numpy())\n",
    "\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            log_p, logdet, _ = model(image)\n",
    "            cum_log_p.append(log_p.cpu().detach().numpy())\n",
    "\n",
    "        logdet = logdet.mean()\n",
    "\n",
    "        loss, log_p, log_det = calc_loss(log_p, logdet, img_size, n_bins, channels=channels)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Loss: {loss.item():.5f}; logP: {log_p.item():.5f}; Avg logP: {np.mean(cum_log_p).item():.5f}; logdet: {log_det.item():.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVHN data loader\n",
    "transformers = transforms.Compose([transforms.ToTensor()\n",
    "                                  ])\n",
    "test_set = datasets.SVHN(\n",
    "        '/nfs/students/ayle/guided-research/gitignored/data',\n",
    "        split='test',\n",
    "        download=True,\n",
    "        transform=transformers\n",
    "    )\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_set,\n",
    "        batch_size=batch,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "dataset = iter(test_loader)\n",
    "len_dataset = len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataloader\n",
    "path = \"/nfs/students/ayle/guided-research/MNIST-jpg/testing\"\n",
    "dataset = iter(sample_data(path, batch, img_size))\n",
    "len_dataset = len(datasets.ImageFolder(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### EVALUATION\n",
    "n_bins = 2.0 ** n_bits\n",
    "\n",
    "z_sample = []\n",
    "z_shapes = calc_z_shapes(channels, img_size, n_flow, n_block)\n",
    "for z in z_shapes:\n",
    "    z_new = torch.randn(n_sample, *z) * temp\n",
    "    z_sample.append(z_new.to(device))\n",
    "    \n",
    "ood_cum_log_p = []\n",
    "\n",
    "with tqdm(range(int(len_dataset / batch))) as pbar:\n",
    "    for i in pbar:\n",
    "        image, _ = next(dataset)\n",
    "        image = image.to(device)\n",
    "                \n",
    "        image = image * 255\n",
    "\n",
    "        if n_bits < 8:\n",
    "            image = torch.floor(image / 2 ** (8 - n_bits))\n",
    "\n",
    "        image = image / n_bins - 0.5\n",
    "\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                log_p, logdet, _ = model.module(\n",
    "                    image\n",
    "                )\n",
    "                ood_cum_log_p.append(log_p.cpu().detach().numpy())\n",
    "\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            log_p, logdet, _ = model(image)\n",
    "            ood_cum_log_p.append(log_p.cpu().detach().numpy())\n",
    "\n",
    "        logdet = logdet.mean()\n",
    "\n",
    "        loss, log_p, log_det = calc_loss(log_p, logdet, img_size, n_bins, channels=channels)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Loss: {loss.item():.5f}; logP: {log_p.item():.5f}; Avg logP: {np.mean(ood_cum_log_p).item():.5f}; logdet: {log_det.item():.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_log_p = np.concatenate([logp for logp in cum_log_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_cum_log_p = np.concatenate([logp for logp in ood_cum_log_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(cum_log_p, density=True, bins=100, histtype='stepfilled', label='In-dist', alpha=0.7)\n",
    "plt.hist(ood_cum_log_p, density=True, bins=100, histtype='stepfilled', label='OOD', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in model.module.mask.items():\n",
    "    print(name)\n",
    "    print((layer == 0).float().sum() / torch.numel(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
