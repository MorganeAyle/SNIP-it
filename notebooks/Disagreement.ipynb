{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/nfs/homedirs/ayle/guided-research/SNIP-it/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import foolbox as fb\n",
    "from experiments.main import load_checkpoint\n",
    "from models import GeneralModel\n",
    "from models.statistics.Metrics import Metrics\n",
    "from utils.config_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.system_utils import *\n",
    "from utils.attacks_utils import get_attack\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from utils.metrics import calculate_aupr, calculate_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arguments = dict({\n",
    "'eval_freq': 1000,  # evaluate every n batches\n",
    "    'save_freq': 1e6,  # save model every n epochs, besides before and after training\n",
    "    'batch_size': 512,  # size of batches, for Imagenette 128\n",
    "    'seed': 1234,  # random seed\n",
    "    'max_training_minutes': 6120 , # one hour and a 45 minutes max, process killed after n minutes (after finish of epoch)\n",
    "    'plot_weights_freq': 50, # plot pictures to tensorboard every n epochs\n",
    "    'prune_freq': 1, # if pruning during training: how long to wait before starting\n",
    "    'prune_delay': 0, # \"if pruning during training: 't' from algorithm box, interval between pruning events, default=0\n",
    "    'prune_to': 0,\n",
    "    'epochs': 0,\n",
    "    'rewind_to': 0, # rewind to this epoch if rewinding is done\n",
    "    'snip_steps': 5, # 's' in algorithm box, number of pruning steps for 'rule of thumb', TODO\n",
    "    'pruning_rate': 0.0, # pruning rate passed to criterion at pruning event. however, most override this\n",
    "    'growing_rate': 0.0000 , # grow back so much every epoch (for future criterions)\n",
    "    'pruning_limit': 0.5,  # Prune until here, if structured in nodes, if unstructured in weights. most criterions use this instead of the pruning_rate\n",
    "    'local_pruning': 0,\n",
    "    'learning_rate': 2e-3,\n",
    "    'grad_clip': 10,\n",
    "    'grad_noise': 0 , # added gaussian noise to gradients\n",
    "    'l2_reg': 5e-5 , # weight decay\n",
    "    'l1_reg': 0 , # l1-norm regularisation\n",
    "    'lp_reg': 0 , # lp regularisation with p < 1\n",
    "    'l0_reg': 1.0 , # l0 reg lambda hyperparam\n",
    "    'hoyer_reg': 0.001 , # hoyer reg lambda hyperparam\n",
    "    'beta_ema': 0.999 , # l0 reg beta ema hyperparam\n",
    "\n",
    "    'loss': 'CrossEntropy',\n",
    "    'optimizer': 'ADAM',\n",
    "    'model': 'ResNet18',  # ResNet not supported with structured\n",
    "    'data_set': 'CIFAR10',\n",
    "    'ood_data_set': 'SVHN',\n",
    "    'ood_data_set_prune': 'SVHN',\n",
    "    'prune_criterion': 'EmptyCrit',  # options: SNIP, SNIPit, SNIPitDuring, UnstructuredRandom, GRASP, HoyerSquare, IMP, // SNAPit, StructuredRandom, GateDecorators, EfficientConvNets, GroupHoyerSquare\n",
    "    'train_scheme': 'DefaultTrainer' , # default: DefaultTrainer\n",
    "    'attack': 'FGSM',\n",
    "    'epsilon': 12,\n",
    "    'eval_ood_data_sets': ['SVHN', 'CIFAR100'],\n",
    "    'eval_attacks': ['FGSM', 'L2FGSM'],\n",
    "    'eval_epsilons': [6, 12, 48],\n",
    "\n",
    "    'device': 'cuda',\n",
    "    'results_dir': \"tmp\",\n",
    "\n",
    "    'checkpoint_name': None,\n",
    "    'checkpoint_model': None,\n",
    "\n",
    "    'disable_cuda_benchmark': 1 , # speedup (disable) vs reproducibility (leave it)\n",
    "    'eval': 0,\n",
    "    'disable_autoconfig': 0 , # for the brave\n",
    "    'preload_all_data': 0 , # load all data into ram memory for speedups\n",
    "    'tuning': 0 , # splits trainset into train and validationset, omits test set\n",
    "\n",
    "    'get_hooks': 0,\n",
    "    'track_weights': 0 , # \"keep statistics on the weights through training\n",
    "    'disable_masking': 1 , # disable the ability to prune unstructured\n",
    "    'enable_rewinding': 0, # enable the ability to rewind to previous weights\n",
    "    'outer_layer_pruning': 1, # allow to prune outer layers (unstructured) or not (structured)\n",
    "    'first_layer_dense': 0,\n",
    "    'random_shuffle_labels': 0  ,# run with random-label experiment from zhang et al\n",
    "    'l0': 0,  # run with l0 criterion, might overwrite some other arguments\n",
    "    'hoyer_square': 0, # \"run in unstructured DeephoyerSquare criterion, might overwrite some other arguments\n",
    "    'group_hoyer_square': 0 ,# run in unstructured Group-DeephoyerSquare criterion, might overwrite some other arguments\n",
    "\n",
    "    'disable_histograms': 0,\n",
    "    'disable_saliency': 0,\n",
    "    'disable_confusion': 0,\n",
    "    'disable_weightplot': 0,\n",
    "    'disable_netplot': 0,\n",
    "    'skip_first_plot': 0,\n",
    "    'disable_activations': 0,\n",
    "    \n",
    "#     'input_dim': [1, 28, 28],\n",
    "#       'output_dim': 10,\n",
    "#       'hidden_dim': [512],\n",
    "#       'N': 60000,\n",
    "    \n",
    "    'input_dim': [3, 32, 32],\n",
    "      'output_dim': 10,\n",
    "      'hidden_dim': [512],\n",
    "      'N': 60000\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sacred import Experiment\n",
    "import numpy as np\n",
    "import seml\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append('.')\n",
    "\n",
    "from models import GeneralModel\n",
    "from models.statistics.Metrics import Metrics\n",
    "from utils.config_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.system_utils import *\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from lipEstimation.lipschitz_utils import compute_module_input_sizes\n",
    "from lipEstimation.lipschitz_approximations import lipschitz_spectral_ub\n",
    "\n",
    "\n",
    "def main(\n",
    "        arguments,\n",
    "        metrics: Metrics\n",
    "):\n",
    "\n",
    "    global out\n",
    "    out = metrics.log_line\n",
    "    out(f\"starting at {get_date_stamp()}\")\n",
    "\n",
    "    # hardware\n",
    "    device = configure_device(arguments)\n",
    "\n",
    "    if arguments['disable_cuda_benchmark']:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # for reproducibility\n",
    "    configure_seeds(arguments, device)\n",
    "\n",
    "    # filter for incompatible properties\n",
    "    assert_compatibilities(arguments)\n",
    "\n",
    "    # load pre-trained weights if specified\n",
    "    path2 = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/results/ResNet18/2021-07-27_05.54.29_model=ResNet18_dataset=CIFAR10_prune-criterion=SNIPit_pruning-limit=0.5_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=2345/models/ResNet18_mod_finished.pickle'\n",
    "    path1 = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/results/ResNet18/2021-07-26_23.33.17_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=2345/models/ResNet18_mod_finished.pickle' \n",
    "\n",
    "    model1 = load_checkpoint(path1).eval()\n",
    "    model2 = load_checkpoint(path2).eval()\n",
    "    ensembles = [model1, model2]\n",
    "\n",
    "    # load data\n",
    "    train_loader, test_loader = find_right_model(\n",
    "        DATASETS, arguments['data_set'],\n",
    "        arguments=arguments\n",
    "    )\n",
    "\n",
    "    # load OOD data\n",
    "    _, ood_loader = find_right_model(\n",
    "        DATASETS, arguments['ood_data_set'],\n",
    "        arguments=arguments\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    out(\"EVALUATING...\")\n",
    "    \n",
    "    kl_loss = nn.KLDivLoss(reduction='none')\n",
    "    kl_preds = np.zeros(0)\n",
    "    true_labels = np.zeros(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, batch in enumerate(test_loader):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            out1 = model1(x)  \n",
    "            out2 = model2(x)\n",
    "            \n",
    "#             kl = kl_loss.forward(torch.log(F.softmax(out1, -1)), F.softmax(out2, -1)).sum(-1)\n",
    "            \n",
    "            kl = kl_loss.forward(torch.log(F.softmax(out2, -1)), F.softmax(out1, -1)).sum(-1)\n",
    "            \n",
    "            kl_preds = np.concatenate((kl_preds, kl.cpu().numpy()))\n",
    "            true_labels = np.concatenate((true_labels, np.zeros(len(x))))\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        for batch_num, batch in enumerate(ood_loader):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            out1 = model1(x)  \n",
    "            out2 = model2(x)\n",
    "            \n",
    "            kl = kl_loss.forward(torch.log(F.softmax(out2, -1)), F.softmax(out1, -1)).sum(-1)\n",
    "            \n",
    "            kl_preds = np.concatenate((kl_preds, kl.cpu().numpy()))\n",
    "            true_labels = np.concatenate((true_labels, np.ones(len(x))))\n",
    "            \n",
    "    auroc = calculate_auroc(true_labels, kl_preds)\n",
    "    aupr = calculate_aupr(true_labels, kl_preds)\n",
    "    \n",
    "    results['auroc'] = auroc\n",
    "    results['aupr'] = aupr\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def assert_compatibilities(arguments):\n",
    "    check_incompatible_props([arguments['loss'] != \"L0CrossEntropy\", arguments['l0']], \"l0\", arguments['loss'])\n",
    "    check_incompatible_props([arguments['train_scheme'] != \"L0Trainer\", arguments['l0']], \"l0\", arguments['train_scheme'])\n",
    "    check_incompatible_props([arguments['l0'], arguments['group_hoyer_square'], arguments['hoyer_square']],\n",
    "                             \"Choose one mode, not multiple\")\n",
    "    check_incompatible_props(\n",
    "        [\"Structured\" in arguments['prune_criterion'], \"Group\" in arguments['prune_criterion'], \"ResNet\" in arguments['model']],\n",
    "        \"structured\", \"residual connections\")\n",
    "    # todo: add more\n",
    "\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "def log_start_run(arguments, out):\n",
    "    arguments.PyTorch_version = torch.__version__\n",
    "    arguments.PyThon_version = sys.version\n",
    "    arguments.pwd = os.getcwd()\n",
    "    out(\"PyTorch version:\", torch.__version__, \"Python version:\", sys.version)\n",
    "    out(\"Working directory: \", os.getcwd())\n",
    "    out(\"CUDA avalability:\", torch.cuda.is_available(), \"CUDA version:\", torch.version.cuda)\n",
    "    out(arguments)\n",
    "\n",
    "def run(arguments):\n",
    "    metrics = Metrics()\n",
    "    out = metrics.log_line\n",
    "    metrics._batch_size = arguments['batch_size']\n",
    "    metrics._eval_freq = arguments['eval_freq']\n",
    "    set_results_dir(arguments[\"results_dir\"])\n",
    "    return main(arguments, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at 2021-08-05_15.24.59\n",
      "Using downloaded and verified file: gitignored/data/train_32x32.mat\n",
      "Using downloaded and verified file: gitignored/data/test_32x32.mat\n",
      "EVALUATING...\n"
     ]
    }
   ],
   "source": [
    "results = run(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auroc': 0.863414153733866, 'aupr': 0.8998101736883848}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
