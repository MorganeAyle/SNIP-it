{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/nfs/homedirs/ayle/guided-research/SNIP-it/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import foolbox as fb\n",
    "from experiments.main import load_checkpoint\n",
    "from models import GeneralModel\n",
    "from models.statistics.Metrics import Metrics\n",
    "from utils.config_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.system_utils import *\n",
    "from utils.attacks_utils import get_attack\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from copy import deepcopy\n",
    "from utils.metrics import calculate_aupr, calculate_auroc\n",
    "from utils.attacks_utils import construct_adversarial_examples\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = dict({\n",
    "'eval_freq': 1000,  # evaluate every n batches\n",
    "    'save_freq': 1e6,  # save model every n epochs, besides before and after training\n",
    "    'batch_size': 512,  # size of batches, for Imagenette 128\n",
    "    'seed': 1234,  # random seed\n",
    "    'max_training_minutes': 6120 , # one hour and a 45 minutes max, process killed after n minutes (after finish of epoch)\n",
    "    'plot_weights_freq': 50, # plot pictures to tensorboard every n epochs\n",
    "    'prune_freq': 1, # if pruning during training: how long to wait before starting\n",
    "    'prune_delay': 0, # \"if pruning during training: 't' from algorithm box, interval between pruning events, default=0\n",
    "    'prune_to': 0,\n",
    "    'epochs': 0,\n",
    "    'rewind_to': 0, # rewind to this epoch if rewinding is done\n",
    "    'snip_steps': 5, # 's' in algorithm box, number of pruning steps for 'rule of thumb', TODO\n",
    "    'snip_iter': 1000,\n",
    "    'pruning_rate': 0.0, # pruning rate passed to criterion at pruning event. however, most override this\n",
    "    'growing_rate': 0.0000 , # grow back so much every epoch (for future criterions)\n",
    "    'pruning_limit': 0.0,  # Prune until here, if structured in nodes, if unstructured in weights. most criterions use this instead of the pruning_rate\n",
    "    'local_pruning': 0,\n",
    "    'learning_rate': 2e-3,\n",
    "    'grad_clip': 10,\n",
    "    'grad_noise': 0 , # added gaussian noise to gradients\n",
    "    'l2_reg': 5e-5 , # weight decay\n",
    "    'l1_reg': 0 , # l1-norm regularisation\n",
    "    'lp_reg': 0 , # lp regularisation with p < 1\n",
    "    'l0_reg': 1.0 , # l0 reg lambda hyperparam\n",
    "    'hoyer_reg': 0.001 , # hoyer reg lambda hyperparam\n",
    "    'beta_ema': 0.999 , # l0 reg beta ema hyperparam\n",
    "\n",
    "    'loss': 'CrossEntropy',\n",
    "    'optimizer': 'ADAM',\n",
    "    'model': 'ResNet18',  # ResNet not supported with structured\n",
    "    'data_set': 'CIFAR10',\n",
    "    'ood_data_set': 'SVHN',\n",
    "    'ood_data_set_prune': 'SVHN',\n",
    "    'prune_criterion': 'SNAP',  # options: SNIP, SNIPit, SNIPitDuring, UnstructuredRandom, GRASP, HoyerSquare, IMP, // SNAPit, StructuredRandom, GateDecorators, EfficientConvNets, GroupHoyerSquare\n",
    "    'train_scheme': 'DefaultTrainer' , # default: DefaultTrainer\n",
    "    'attack': 'FGSM',\n",
    "    'epsilon': 6,\n",
    "    'eval_ood_data_sets': ['SVHN', 'CIFAR100'],\n",
    "    'eval_attacks': ['FGSM'],\n",
    "    'eval_epsilons': [8, 48],\n",
    "\n",
    "    'device': 'cuda',\n",
    "    'results_dir': \"tmp\",\n",
    "\n",
    "    'checkpoint_name': None,\n",
    "    'checkpoint_model': None,\n",
    "\n",
    "    'disable_cuda_benchmark': 1 , # speedup (disable) vs reproducibility (leave it)\n",
    "    'eval': 0,\n",
    "    'disable_autoconfig': 0 , # for the brave\n",
    "    'preload_all_data': 0 , # load all data into ram memory for speedups\n",
    "    'tuning': 0 , # splits trainset into train and validationset, omits test set\n",
    "\n",
    "    'get_hooks': 0,\n",
    "    'track_weights': 0 , # \"keep statistics on the weights through training\n",
    "    'disable_masking': 1 , # disable the ability to prune unstructured\n",
    "    'enable_rewinding': 0, # enable the ability to rewind to previous weights\n",
    "    'outer_layer_pruning': 1, # allow to prune outer layers (unstructured) or not (structured)\n",
    "    'first_layer_dense': 0,\n",
    "    'random_shuffle_labels': 0  ,# run with random-label experiment from zhang et al\n",
    "    'l0': 0,  # run with l0 criterion, might overwrite some other arguments\n",
    "    'hoyer_square': 0, # \"run in unstructured DeephoyerSquare criterion, might overwrite some other arguments\n",
    "    'group_hoyer_square': 0 ,# run in unstructured Group-DeephoyerSquare criterion, might overwrite some other arguments\n",
    "\n",
    "    'disable_histograms': 0,\n",
    "    'disable_saliency': 0,\n",
    "    'disable_confusion': 0,\n",
    "    'disable_weightplot': 0,\n",
    "    'disable_netplot': 0,\n",
    "    'skip_first_plot': 0,\n",
    "    'disable_activations': 0,\n",
    "    \n",
    "#     'input_dim': [1, 28, 28],\n",
    "#       'output_dim': 10,\n",
    "#       'hidden_dim': [512],\n",
    "#       'N': 60000,\n",
    "    \n",
    "    'input_dim': [3, 32, 32],\n",
    "    'output_dim': 10,\n",
    "    'hidden_dim': [512],\n",
    "    'N': 60000,\n",
    "    'mean': (0.4914, 0.4822, 0.4465),\n",
    "    'std': (0.2471, 0.2435, 0.2616)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/nfs/students/ayle/guided-research/gitignored/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()\n",
    "out = metrics.log_line\n",
    "metrics._batch_size = arguments['batch_size']\n",
    "metrics._eval_freq = arguments['eval_freq']\n",
    "set_results_dir(arguments[\"results_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "model: GeneralModel = find_right_model(\n",
    "        NETWORKS_DIR, arguments['model'],\n",
    "        device=arguments['device'],\n",
    "        hidden_dim=arguments['hidden_dim'],\n",
    "        input_dim=arguments['input_dim'],\n",
    "        output_dim=arguments['output_dim'],\n",
    "        is_maskable=arguments['disable_masking'],\n",
    "        is_tracking_weights=arguments['track_weights'],\n",
    "        is_rewindable=arguments['enable_rewinding'],\n",
    "        is_growable=arguments['growing_rate'] > 0,\n",
    "        outer_layer_pruning=arguments['outer_layer_pruning'],\n",
    "        maintain_outer_mask_anyway=(\n",
    "                                       not arguments['outer_layer_pruning']) and (\n",
    "                                           \"Structured\" in arguments['prune_criterion']),\n",
    "        l0=arguments['l0'],\n",
    "        l0_reg=arguments['l0_reg'],\n",
    "        N=arguments['N'],\n",
    "        beta_ema=arguments['beta_ema'],\n",
    "        l2_reg=arguments['l2_reg']\n",
    ").to(arguments['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(arguments, model, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, model, out):\n",
    "    with open(path, 'rb') as f:\n",
    "        state = pickle.load(f)\n",
    "    try:\n",
    "        model.load_state_dict(state)\n",
    "    except KeyError as e:\n",
    "        print(list(state.keys()))\n",
    "        raise e\n",
    "    out(f\"Loaded checkpoint {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint /nfs/students/ayle/guided-research/results/ResNet18/2021-07-26_22.46.19_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=1234/models/ResNet18_finished.pickle\n"
     ]
    }
   ],
   "source": [
    "# path = '/nfs/students/ayle/guided-research/results/Conv6/2021-07-12_03.45.39_model=Conv6_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=1234/models/Conv6_finished.pickle'\n",
    "# path = '/nfs/students/ayle/guided-research/results/Conv6/2021-07-12_04.48.17_model=Conv6_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=2345/models/Conv6_finished.pickle'\n",
    "# path = '/nfs/students/ayle/guided-research/results/Conv6/2021-07-15_19.21.19_model=Conv6_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=3456/models/Conv6_finished.pickle'\n",
    "# path = '/nfs/students/ayle/guided-research/results/Conv6/2021-07-15_19.25.40_model=Conv6_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=4567/models/Conv6_finished.pickle'\n",
    "# path = '/nfs/students/ayle/guided-research/results/Conv6/2021-07-15_19.25.40_model=Conv6_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=4567/models/Conv6_finished.pickle'\n",
    "\n",
    "# path = '/nfs/students/ayle/guided-research/results/LeNet5/2021-07-11_03.10.29_model=LeNet5_dataset=FASHION_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=1234/models/LeNet5_finished.pickle'\n",
    "\n",
    "# path = '/nfs/students/ayle/guided-research/results/ResNet18/2021-07-13_11.03.15_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=1234/models/ResNet18_finished.pickle'\n",
    "\n",
    "path = '/nfs/students/ayle/guided-research/results/ResNet18/2021-07-26_22.46.19_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=1234/models/ResNet18_finished.pickle'\n",
    "# path2 = '/nfs/students/ayle/guided-research/results/ResNet18/2021-07-26_23.33.17_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=2345/models/ResNet18_finished.pickle'\n",
    "# path = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/results/ResNet18/2021-07-26_23.35.18_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=3456/models/ResNet18_finished.pickle'\n",
    "# path = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/results/ResNet18/2021-07-26_23.35.46_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=4567/models/ResNet18_finished.pickle'\n",
    "# path = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/results/ResNet18/2021-07-26_23.36.23_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=5678/models/ResNet18_finished.pickle'\n",
    "\n",
    "# path = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/results/VGG16/2021-08-22_11.02.10_model=VGG16_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=1234/models/VGG16_finished.pickle'\n",
    "\n",
    "# path = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/results/ResNet18/2021-08-29_13.57.22_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=1234/models/ResNet18_finished.pickle'\n",
    "\n",
    "# path = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/results/ResNet18/2021-08-29_18.19.04_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=1234/models/ResNet18_finished.pickle'\n",
    "\n",
    "# path = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/results/ResNet18/2021-08-24_11.36.47_model=ResNet18_dataset=CIFAR10_prune-criterion=EarlyJohn_pruning-limit=0.94_prune-freq=1_prune-delay=0_outer-layer-pruning=1_prune-to=10_rewind-to=0_train-scheme=DefaultTrainer_seed=1234/models/ResNet18_finished.pickle'\n",
    "\n",
    "# path = '/nfs/students/ayle/guided-research/gitignored/results/tests/2021-09-21_10.19.16_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_train-scheme=DefaultTrainer_seed=1234/models/ResNet18_finished.pickle'\n",
    "\n",
    "# model trained on augerino augmentations\n",
    "# path = '/nfs/students/ayle/guided-research/gitignored/results/tests/2021-09-24_11.30.01_model=ResNet18_dataset=CIFAR10_prune-criterion=EmptyCrit_pruning-limit=0.0_train-scheme=DefaultTrainer_seed=1234/models/ResNet18_finished.pickle'\n",
    "\n",
    "# augerino\n",
    "# path = '/nfs/students/ayle/guided-research/gitignored/results/invariances/aug_no_trans_trained.pt'\n",
    "\n",
    "load_checkpoint(path, model, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = arguments['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # augmentation stuff\n",
    "# width = torch.load(\"/nfs/students/ayle/guided-research/gitignored/results/invariances/aug_no_trans_trained_width.pt\")\n",
    "\n",
    "# from augerino import models \n",
    "# aug = models.UniformAug()\n",
    "# aug.set_width(width.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean (0.4914, 0.4822, 0.4465)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/homedirs/ayle/miniconda3/envs/inv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean (0.4914, 0.4822, 0.4465)\n",
      "Using downloaded and verified file: /nfs/students/ayle/guided-research/gitignored/data/train_32x32.mat\n",
      "Using downloaded and verified file: /nfs/students/ayle/guided-research/gitignored/data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_loader, test_loader = find_right_model(\n",
    "    DATASETS, arguments['data_set'],\n",
    "    arguments=arguments,\n",
    "    mean=arguments['mean'],\n",
    "    std=arguments['std']\n",
    ")\n",
    "\n",
    "# load OOD data\n",
    "_, ood_loader = find_right_model(\n",
    "    DATASETS, arguments['ood_data_set'],\n",
    "    arguments=arguments,\n",
    "    mean=arguments['mean'],\n",
    "    std=arguments['std']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean (0.4914, 0.4822, 0.4465)\n",
      "Using downloaded and verified file: /nfs/students/ayle/guided-research/gitignored/data/train_32x32.mat\n",
      "Using downloaded and verified file: /nfs/students/ayle/guided-research/gitignored/data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# load OOD data\n",
    "ood_prune_loader, _ = find_right_model(\n",
    "    DATASETS, arguments['ood_data_set_prune'],\n",
    "    arguments=arguments,\n",
    "    mean=arguments['mean'],\n",
    "    std=arguments['std']\n",
    ")\n",
    "\n",
    "# get loss function\n",
    "loss = find_right_model(\n",
    "    LOSS_DIR, arguments['loss'],\n",
    "    device=device,\n",
    "    l1_reg=arguments['l1_reg'],\n",
    "    lp_reg=arguments['lp_reg'],\n",
    "    l0_reg=arguments['l0_reg'],\n",
    "    hoyer_reg=arguments['hoyer_reg']\n",
    ")\n",
    "\n",
    "# get optimizer\n",
    "optimizer = find_right_model(\n",
    "    OPTIMS, arguments['optimizer'],\n",
    "    params=model.parameters(),\n",
    "    lr=arguments['learning_rate'],\n",
    "    weight_decay=arguments['l2_reg'] if not arguments['l0'] else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made datestamp: 2021-09-29_18.48.15_model=ResNet18_dataset=CIFAR10_ood-dataset=SVHN_attack=FGSM_epsilon=6_prune-criterion=SNAP_pruning-limit=0.0_prune-freq=1_prune-delay=0_rewind-to=0_train-scheme=DefaultTrainer_seed=1234\n",
      "\u001b[1mStarted training\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run_name = f'_model={arguments[\"model\"]}_dataset={arguments[\"data_set\"]}_ood-dataset={arguments[\"ood_data_set\"]}' + \\\n",
    "           f'_attack={arguments[\"attack\"]}_epsilon={arguments[\"epsilon\"]}_prune-criterion={arguments[\"prune_criterion\"]}' + \\\n",
    "           f'_pruning-limit={arguments[\"pruning_limit\"]}_prune-freq={arguments[\"prune_freq\"]}_prune-delay={arguments[\"prune_delay\"]}' + \\\n",
    "           f'_rewind-to={arguments[\"rewind_to\"]}_train-scheme={arguments[\"train_scheme\"]}_seed={arguments[\"seed\"]}'\n",
    "\n",
    "\n",
    "criterion = find_right_model(\n",
    "        CRITERION_DIR, arguments['prune_criterion'],\n",
    "        model=model,\n",
    "        limit=arguments['pruning_limit'],\n",
    "        start=0.5,\n",
    "        orig_scores=True,\n",
    "        steps=arguments['snip_steps'],\n",
    "        device=arguments['device'],\n",
    "        arguments=arguments,\n",
    "    )\n",
    "\n",
    "# build trainer\n",
    "trainer = find_right_model(\n",
    "    TRAINERS_DIR, arguments['train_scheme'],\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    arguments=arguments,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    ood_loader=ood_loader,\n",
    "    ood_prune_loader=ood_prune_loader,\n",
    "    metrics=metrics,\n",
    "    criterion=criterion,\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_grads = criterion.grads_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SNAP' object has no attribute 'scores_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9a1c6e0ed8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morig_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0morig_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/inv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    948\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SNAP' object has no attribute 'scores_mean'"
     ]
    }
   ],
   "source": [
    "orig_mean = criterion.scores_mean\n",
    "orig_std = criterion.scores_std\n",
    "layer_names = list(orig_grads.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, val in orig_grads.items():\n",
    "#     orig_grads[name] = (val - orig_mean[name]) / (1e-8 + orig_std[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean (0.4914, 0.4822, 0.4465)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using mean (0.4914, 0.4822, 0.4465)\n",
      "Using downloaded and verified file: /nfs/students/ayle/guided-research/gitignored/data/train_32x32.mat\n",
      "Using downloaded and verified file: /nfs/students/ayle/guided-research/gitignored/data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "arguments['batch_size'] = 2\n",
    "\n",
    "# load data\n",
    "train_loader, test_loader = find_right_model(\n",
    "    DATASETS, arguments['data_set'],\n",
    "    arguments=arguments,\n",
    "    mean=arguments['mean'],\n",
    "    std=arguments['std']\n",
    ")\n",
    "\n",
    "# load OOD data\n",
    "_, ood_loader = find_right_model(\n",
    "    DATASETS, arguments['ood_data_set'],\n",
    "    arguments=arguments,\n",
    "    mean=arguments['mean'],\n",
    "    std=arguments['std']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = torch.nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 500/5000 [00:55<08:15,  9.08it/s]\n"
     ]
    }
   ],
   "source": [
    "norms = []\n",
    "per_layer_norms = []\n",
    "for i, (x, y) in enumerate(tqdm(test_loader)):\n",
    "    if i == int(len(test_loader)*0.1): break\n",
    "    \n",
    "    model = deepcopy(backup_model)\n",
    "    model.eval()\n",
    "\n",
    "    x = x.cuda()\n",
    "\n",
    "#     new_x = [x]\n",
    "#     for im in x:\n",
    "#         for _ in range(5):\n",
    "#             image = aug(im.unsqueeze(0))\n",
    "#             new_x.append(image)\n",
    "#     x = torch.cat(new_x)\n",
    "\n",
    "    out = model(x)\n",
    "    preds = out.argmax(dim=-1, keepdim=True).flatten()\n",
    "\n",
    "    train_loader = [(x, preds)]\n",
    "\n",
    "#     train_loader = []\n",
    "#     for im in x:\n",
    "#         batch = []\n",
    "        \n",
    "#         batch.append(im.unsqueeze(0))\n",
    "#         for _ in range(1):\n",
    "#             new_x = aug(deepcopy(im.unsqueeze(0)))\n",
    "#             batch.append(new_x)\n",
    "#         batch = torch.cat(batch)\n",
    "#         out = model(batch)\n",
    "#         pred = out.argmax(dim=-1, keepdim=True).flatten()\n",
    "        \n",
    "#         train_loader.append((batch, pred))\n",
    "        \n",
    "    # get criterion\n",
    "    criterion = find_right_model(\n",
    "        CRITERION_DIR, arguments['prune_criterion'],\n",
    "        model=model,\n",
    "        limit=arguments['pruning_limit'],\n",
    "        start=0.5,\n",
    "        steps=arguments['snip_steps'],\n",
    "        device=arguments['device'],\n",
    "        arguments=arguments\n",
    "    )\n",
    "    \n",
    "    criterion.prune(arguments['pruning_limit'],\n",
    "                        train_loader=train_loader,\n",
    "                      ood_loader=None,\n",
    "                      local=arguments['local_pruning'],\n",
    "                      manager=None)\n",
    "    \n",
    "    layer_norms = []\n",
    "    for j, (grad1, grad2) in enumerate(zip(orig_grads.values(), criterion.grads_abs.values())):\n",
    "        if j == 0: continue #####################################################################\n",
    "#         grad3 = (grad2 - orig_mean[layer_names[j]]) /  (1e-8 + orig_std[layer_names[j]])\n",
    "        grad3 = grad2\n",
    "#         layer_norms.append(torch.norm(grad1 - grad3, p=2).cpu().detach().numpy())\n",
    "#         layer_norms.append(cosine(grad1.cpu().detach().numpy(), grad3.cpu().detach().numpy()))\n",
    "        layer_norms.append(kl(torch.log(torch.nn.functional.softmax(grad1, -1)).unsqueeze(0), torch.nn.functional.softmax(grad3, -1).unsqueeze(0)).cpu().numpy())\n",
    "#         layer_norms.append(MMD(grad1.unsqueeze(0), grad3.unsqueeze(0), kernel=\"rbf\").cpu().detach().numpy())\n",
    "    per_layer_norms.append(layer_norms)\n",
    "    norms.append(np.mean(layer_norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean (0.4914, 0.4822, 0.4465)\n",
      "Using downloaded and verified file: /nfs/students/ayle/guided-research/gitignored/data/train_32x32.mat\n",
      "Using downloaded and verified file: /nfs/students/ayle/guided-research/gitignored/data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# Load OOD data\n",
    "_, ood_loader = find_right_model(\n",
    "    DATASETS, \"SVHN\",\n",
    "    arguments=arguments,\n",
    "    mean=arguments['mean'],\n",
    "    std=arguments['std']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 500/13016 [00:58<24:16,  8.59it/s] \n"
     ]
    }
   ],
   "source": [
    "# OOD data\n",
    "\n",
    "ood_norms = []\n",
    "ood_per_layer_norms = []\n",
    "\n",
    "for i, (x, y) in enumerate(tqdm(ood_loader)):\n",
    "    if i == int(len(test_loader)*0.1): break\n",
    "    \n",
    "    model = deepcopy(backup_model)\n",
    "    model.eval()\n",
    "\n",
    "    x = x.cuda()\n",
    "\n",
    "#     new_x = [x]\n",
    "#     for im in x:\n",
    "#         for _ in range(5):\n",
    "#             image = aug(im.unsqueeze(0))\n",
    "#             new_x.append(image)\n",
    "#     x = torch.cat(new_x)\n",
    "\n",
    "    out = model(x)\n",
    "    preds = out.argmax(dim=-1, keepdim=True).flatten()\n",
    "\n",
    "    train_loader = [(x, preds)]\n",
    "\n",
    "#     train_loader = []\n",
    "#     for im in x:\n",
    "#         batch = []\n",
    "        \n",
    "#         batch.append(im.unsqueeze(0))\n",
    "#         for _ in range(1):\n",
    "#             new_x = aug(deepcopy(im.unsqueeze(0)))\n",
    "#             batch.append(new_x)\n",
    "#         batch = torch.cat(batch)\n",
    "#         out = model(batch)\n",
    "#         pred = out.argmax(dim=-1, keepdim=True).flatten()\n",
    "        \n",
    "#         train_loader.append((batch, pred))\n",
    "    \n",
    "    # get criterion\n",
    "    criterion = find_right_model(\n",
    "        CRITERION_DIR, arguments['prune_criterion'],\n",
    "        model=model,\n",
    "        limit=arguments['pruning_limit'],\n",
    "        start=0.5,\n",
    "        steps=arguments['snip_steps'],\n",
    "        device=arguments['device'],\n",
    "        arguments=arguments\n",
    "    )\n",
    "    \n",
    "    criterion.prune(arguments['pruning_limit'],\n",
    "                        train_loader=train_loader,\n",
    "                      ood_loader=None,\n",
    "                      local=arguments['local_pruning'],\n",
    "                      manager=None)\n",
    "    \n",
    "    layer_norms = []\n",
    "    for j, (grad1, grad2) in enumerate(zip(orig_grads.values(), criterion.grads_abs.values())):\n",
    "        if j == 0: continue #####################################################################\n",
    "#         grad3 = (grad2 - orig_mean[layer_names[j]]) /  (1e-8 + orig_std[layer_names[j]])\n",
    "        grad3 = grad2\n",
    "#         layer_norms.append(torch.norm(grad1 - grad3, p=2).cpu().detach().numpy())\n",
    "#         layer_norms.append(cosine(grad1.cpu().detach().numpy(), grad3.cpu().detach().numpy()))\n",
    "        layer_norms.append(kl(torch.log(torch.nn.functional.softmax(grad1, -1)).unsqueeze(0), torch.nn.functional.softmax(grad3, -1).unsqueeze(0)).cpu().numpy())\n",
    "#         layer_norms.append(MMD(grad1.unsqueeze(0), grad3.unsqueeze(0), kernel=\"rbf\").cpu().detach().numpy())\n",
    "    per_layer_norms.append(layer_norms)\n",
    "    ood_norms.append(np.mean(layer_norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attacks\n",
    "from torchvision import datasets\n",
    "from utils.constants import DATASET_PATH\n",
    "mean=arguments['mean']\n",
    "std=arguments['std']\n",
    "test_set = datasets.CIFAR10(root=DATASET_PATH, train=False, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_set,\n",
    "        batch_size=arguments['batch_size'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "trans = transforms.Normalize(mean, std)\n",
    "\n",
    "# Attacks\n",
    "ood_norms = []\n",
    "attack_per_layer_norms = []\n",
    "\n",
    "for i, (x, y) in enumerate(tqdm(test_loader)):\n",
    "    if i == int(len(test_loader)*0.1): break\n",
    "    \n",
    "    model = deepcopy(backup_model)\n",
    "    model.eval()\n",
    "        \n",
    "    adv_results, predictions = construct_adversarial_examples(x, y, 'FGSM', model, model.device, 8, False, False)\n",
    "    _, advs, success = adv_results\n",
    "    x = advs.cpu()\n",
    "        \n",
    "    new_x = []\n",
    "    for image in x:\n",
    "        image = trans(image.squeeze()).unsqueeze(0)\n",
    "        new_x.append(image)\n",
    "    x = torch.cat(new_x)\n",
    "    \n",
    "    x = x.cuda()\n",
    "    \n",
    "#     new_x = [x]\n",
    "#     for im in x:\n",
    "#         for _ in range(5):\n",
    "#             image = aug(im.unsqueeze(0))\n",
    "#             new_x.append(image)\n",
    "#     x = torch.cat(new_x)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    out = model(x.cuda())\n",
    "    preds = out.argmax(dim=-1, keepdim=True).flatten()\n",
    "    \n",
    "    # get criterion\n",
    "    criterion = find_right_model(\n",
    "        CRITERION_DIR, arguments['prune_criterion'],\n",
    "        model=model,\n",
    "        limit=arguments['pruning_limit'],\n",
    "        start=0.5,\n",
    "        steps=arguments['snip_steps'],\n",
    "        device=arguments['device'],\n",
    "        arguments=arguments\n",
    "    )\n",
    "    \n",
    "    criterion.prune(arguments['pruning_limit'],\n",
    "                    train_loader=[(x, preds)],\n",
    "                      ood_loader=None,\n",
    "                      local=arguments['local_pruning'],\n",
    "                      manager=None)\n",
    "    \n",
    "    layer_norms = []\n",
    "    for j, (grad1, grad2) in enumerate(zip(orig_grads.values(), criterion.grads_abs.values())):\n",
    "        grad3 = (grad2.cpu() - orig_mean[layer_names[j]]) /  (1e-8 + orig_std[layer_names[j]])\n",
    "#         grad3 = grad2\n",
    "        layer_norms.append(torch.norm(grad1 - grad3, p=5).cpu().detach().numpy())\n",
    "    attack_per_layer_norms.append(layer_norms)\n",
    "    ood_norms.append(np.mean(layer_norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS\n",
    "class DS(Dataset):\n",
    "\n",
    "    def __init__(self, images, labels, mean, std):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.mean = [0.4914, 0.4822, 0.4465]\n",
    "        self.std = [0.2471, 0.2435, 0.2616]\n",
    "        self.transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=self.mean, std=self.std)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = self.images[item] / 255\n",
    "        image = self.transforms(image.transpose((1, 2, 0)))\n",
    "        return image.to(torch.float32), torch.tensor(self.labels[item], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "ds_path = os.path.join(DATASET_PATH, \"cifar10_corrupted\")\n",
    "aurocs = []\n",
    "ds_per_layer_norms = []\n",
    "        \n",
    "for ds_dataset_name in os.listdir(ds_path):\n",
    "    if ds_dataset_name.endswith('5.npz'):\n",
    "        npz_dataset = np.load(os.path.join(ds_path, ds_dataset_name))\n",
    "\n",
    "        ds_dataset = CIFAR10C(npz_dataset[\"images\"], npz_dataset[\"labels\"], arguments[\"mean\"], arguments[\"std\"])\n",
    "        ds_loader = torch.utils.data.DataLoader(\n",
    "            ds_dataset,\n",
    "            batch_size=arguments['batch_size'],\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        ood_norms = []\n",
    "        for i, (x, y) in enumerate(ds_loader):\n",
    "            \n",
    "            model = deepcopy(backup_model)\n",
    "            model.eval()\n",
    "            \n",
    "            out = model(x.cuda())\n",
    "            preds = out.argmax(dim=-1, keepdim=True).view_as(y)\n",
    "\n",
    "            # get criterion\n",
    "            criterion = find_right_model(\n",
    "                CRITERION_DIR, arguments['prune_criterion'],\n",
    "                model=model,\n",
    "                limit=arguments['pruning_limit'],\n",
    "                start=0.5,\n",
    "                steps=arguments['snip_steps'],\n",
    "                device=arguments['device'],\n",
    "                arguments=arguments\n",
    "            )\n",
    "\n",
    "            criterion.prune(arguments['pruning_limit'],\n",
    "                              train_loader=[(x, preds)],\n",
    "                              ood_loader=None,\n",
    "                              local=arguments['local_pruning'],\n",
    "                              manager=None)\n",
    "\n",
    "            layer_norms = []\n",
    "            for grad1, grad2 in zip(orig_grads.values(), criterion.grads_abs.values()):\n",
    "                grad3 = (grad2.cpu() - orig_mean[layer_names[j]]) /  (1e-8 + orig_std[layer_names[j]])\n",
    "#                 grad3 = grad2\n",
    "                layer_norms.append(torch.norm(grad1 - grad3, p=5).cpu().detach().numpy())\n",
    "            ds_per_layer_norms.append(layer_norms)\n",
    "            ood_norms.append(np.mean(layer_norms))\n",
    "\n",
    "        auroc = calculate_auroc(np.concatenate((np.zeros_like(norms), np.ones_like(ood_norms))), np.concatenate((norms, ood_norms)))\n",
    "        print('AUROC', auroc)\n",
    "        aurocs.append(auroc)\n",
    "        \n",
    "print('Mean AUROC', np.mean(aurocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21531701"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ood_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.array(norms)\n",
    "ood_norms = np.array(ood_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230079999999999"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_auroc(np.concatenate((np.zeros_like(norms), np.ones_like(ood_norms))), np.concatenate((norms, ood_norms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(per_layer_norms).mean(0), label='In-distribution')\n",
    "plt.plot(np.array(ood_per_layer_norms).mean(0), label='Out-Of-Distribution')\n",
    "# plt.plot(np.array(attack_per_layer_norms).mean(0), label='Attack (FGSM \\u03B5 = 8)')\n",
    "# plt.plot(np.array(ds_per_layer_norms).mean(0), label='Distribution Shifts')\n",
    "plt.legend()\n",
    "plt.xlabel('ResNet18 layers')\n",
    "plt.ylabel('L2 distance to original scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(ood_per_layer_norms).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds, _ = torch.topk(torch.tensor(norms), int(len(norms)*0.05), sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = thresholds[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_detected = 0\n",
    "for ood_norm in ood_norms:\n",
    "    if ood_norm >= threshold:\n",
    "        num_detected += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_detected / len(ood_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def MMD(x, y, kernel):\n",
    "    \"\"\"Emprical maximum mean discrepancy. The lower the result\n",
    "       the more evidence that distributions are the same.\n",
    "\n",
    "    Args:\n",
    "        x: first sample, distribution P\n",
    "        y: second sample, distribution Q\n",
    "        kernel: kernel type such as \"multiscale\" or \"rbf\"\n",
    "    \"\"\"\n",
    "    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
    "    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n",
    "    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n",
    "    \n",
    "    dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n",
    "    dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n",
    "    dxy = rx.t() + ry - 2. * zz # Used for C in (1)\n",
    "    \n",
    "    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device))\n",
    "    \n",
    "    if kernel == \"multiscale\":\n",
    "        \n",
    "        bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n",
    "        for a in bandwidth_range:\n",
    "            XX += a**2 * (a**2 + dxx)**-1\n",
    "            YY += a**2 * (a**2 + dyy)**-1\n",
    "            XY += a**2 * (a**2 + dxy)**-1\n",
    "            \n",
    "    if kernel == \"rbf\":\n",
    "      \n",
    "        bandwidth_range = [10, 15, 20, 50]\n",
    "        for a in bandwidth_range:\n",
    "            XX += torch.exp(-0.5*dxx/a)\n",
    "            YY += torch.exp(-0.5*dyy/a)\n",
    "            XY += torch.exp(-0.5*dxy/a)\n",
    "      \n",
    "      \n",
    "\n",
    "    return torch.mean(XX + YY - 2. * XY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
