{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/nfs/homedirs/ayle/guided-research/SNIP-it/gitignored/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "'eval_freq': 1000,  # evaluate every n batches\n",
    "    'save_freq': 1e6,  # save model every n epochs, besides before and after training\n",
    "    'batch_size': 512,  # size of batches, for Imagenette 128\n",
    "    'seed': 333,  # random seed\n",
    "    'max_training_minutes': 6120 , # one hour and a 45 minutes max, process killed after n minutes (after finish of epoch)\n",
    "    'plot_weights_freq': 50 , # plot pictures to tensorboard every n epochs\n",
    "    'prune_freq': 1 , # if pruning during training: how long to wait before starting\n",
    "    'prune_delay': 0 , # \"if pruning during training: 't' from algorithm box, interval between pruning events, default=0\n",
    "    'epochs': 80,\n",
    "    'rewind_to': 0 , # rewind to this epoch if rewinding is done\n",
    "    'snip_steps': 5 , # 's' in algorithm box, number of pruning steps for 'rule of thumb', TODO\n",
    "    'pruning_rate': 0.00 , # pruning rate passed to criterion at pruning event. however, most override this\n",
    "    'growing_rate': 0.0000 , # grow back so much every epoch (for future criterions)\n",
    "    'pruning_limit': 0.00,  # Prune until here, if structured in nodes, if unstructured in weights. most criterions use this instead of the pruning_rate\n",
    "    'learning_rate': 2e-3,\n",
    "    'grad_clip': 10,\n",
    "    'grad_noise': 0 , # added gaussian noise to gradients\n",
    "    'l2_reg': 5e-5 , # weight decay\n",
    "    'l1_reg': 0 , # l1-norm regularisation\n",
    "    'lp_reg': 0 , # lp regularisation with p < 1\n",
    "    'l0_reg': 1.0 , # l0 reg lambda hyperparam\n",
    "    'hoyer_reg': 1.0 , # hoyer reg lambda hyperparam\n",
    "    'beta_ema': 0.999 , # l0 reg beta ema hyperparam\n",
    "\n",
    "    'loss': 'CrossEntropy',\n",
    "    'optimizer': 'ADAM',\n",
    "    'model': 'LeNet5',  # ResNet not supported with structured\n",
    "    'data_set': 'MNIST',\n",
    "    'ood_data_set': 'FASHION',\n",
    "    'prune_criterion': 'EmptyCrit',  # options: SNIP, SNIPit, SNIPitDuring, UnstructuredRandom, GRASP, HoyerSquare, IMP, // SNAPit, StructuredRandom, GateDecorators, EfficientConvNets, GroupHoyerSquare\n",
    "    'train_scheme': 'DefaultTrainer' , # default: DefaultTrainer\n",
    "    'test_scheme': 'AdversarialEvaluation'  ,# only supported on unstructured?\n",
    "    'attack': 'FGSM',\n",
    "    'epsilons': [0.25],\n",
    "\n",
    "    'device': 'cuda',\n",
    "    'run_name': \"\",\n",
    "\n",
    "    'checkpoint_name': '2021-04-25_16.04.37_lenet5_dense',\n",
    "    'checkpoint_model': 'LeNet5_finished',\n",
    "\n",
    "    'disable_cuda_benchmark': 1 , # speedup (disable) vs reproducibility (leave it)\n",
    "    'eval': 0,\n",
    "    'disable_autoconfig': 0 , # for the brave\n",
    "    'preload_all_data': 0 , # load all data into ram memory for speedups\n",
    "    'tuning': 0 , # splits trainset into train and validationset, omits test set\n",
    "\n",
    "    'track_weights': 0 , # \"keep statistics on the weights through training\n",
    "    'disable_masking': 1 , # disable the ability to prune unstructured\n",
    "    'enable_rewinding': 1 , # enable the ability to rewind to previous weights\n",
    "    'outer_layer_pruning': 1 , # allow to prune outer layers (unstructured) or not (structured)\n",
    "    'random_shuffle_labels': 0  ,# run with random-label experiment from zhang et al\n",
    "    'l0': 0,  # run with l0 criterion, might overwrite some other arguments\n",
    "    'hoyer_square': 0 , # \"run in unstructured DeephoyerSquare criterion, might overwrite some other arguments\n",
    "    'group_hoyer_square': 0 ,# run in unstructured Group-DeephoyerSquare criterion, might overwrite some other arguments\n",
    "\n",
    "    'disable_histograms': 1,\n",
    "    'disable_saliency': 1,\n",
    "    'disable_confusion': 1,\n",
    "    'disable_weightplot': 1,\n",
    "    'disable_netplot': 1,\n",
    "    'skip_first_plot': 1,\n",
    "    \n",
    "    'input_dim': [1, 28, 28],\n",
    "      'output_dim': 10,\n",
    "      'hidden_dim': [512],\n",
    "      'N': 60000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = datasets.KMNIST(\n",
    "    DATASET_PATH,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transformers\n",
    ")\n",
    "# test_set = datasets.SVHN(\n",
    "#     DATASET_PATH,\n",
    "#     split='test',\n",
    "#     download=True,\n",
    "#     transform=transformers\n",
    "# )\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=arguments['batch_size'],\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=6\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "h, w = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "mean: tensor([0.1918], device='cuda:0')\n",
      "std: tensor([0.3483], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    if batch_idx == 0:\n",
    "        h, w = inputs.size(2), inputs.size(3)\n",
    "        print(inputs.min(), inputs.max())\n",
    "        chsum = inputs.sum(dim=(0, 2, 3), keepdim=True)\n",
    "    else:\n",
    "        chsum += inputs.sum(dim=(0, 2, 3), keepdim=True)\n",
    "mean = chsum/len(train_set)/h/w\n",
    "print('mean: %s' % mean.view(-1))\n",
    "\n",
    "chsum = None\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    if batch_idx == 0:\n",
    "        chsum = (inputs - mean).pow(2).sum(dim=(0, 2, 3), keepdim=True)\n",
    "    else:\n",
    "        chsum += (inputs - mean).pow(2).sum(dim=(0, 2, 3), keepdim=True)\n",
    "std = torch.sqrt(chsum/(len(train_set) * h * w - 1))\n",
    "print('std: %s' % std.view(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
